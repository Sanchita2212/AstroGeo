{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4513effc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "\n",
    "# LangChain and ML imports\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader, UnstructuredHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Visualization imports (optional)\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7eaac140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OpenAI API key loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Model configuration - using cost-effective model\n",
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\"\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(override=True)\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY', 'your-key-if-not-using-env')\n",
    "\n",
    "# Verify API key is loaded\n",
    "if not os.getenv('OPENAI_API_KEY'):\n",
    "    print(\"‚ö†Ô∏è Warning: OpenAI API key not found!\")\n",
    "else:\n",
    "    print(\"‚úÖ OpenAI API key loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "495527db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Document loading functions for different platforms\n",
    "def load_txt_folder(path, site_tag):\n",
    "    \"\"\"Load text files from a directory\"\"\"\n",
    "    try:\n",
    "        loader = DirectoryLoader(\n",
    "            path=path,\n",
    "            glob=\"**/*.txt\",\n",
    "            loader_cls=lambda p: TextLoader(p, encoding=\"utf-8\")\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        for d in docs:\n",
    "            d.metadata[\"source_site\"] = site_tag\n",
    "        print(f\"‚úÖ Loaded {len(docs)} TXT files from {site_tag}\")\n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading TXT from {path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def load_html_folder(path, site_tag):\n",
    "    \"\"\"Load HTML files from a directory\"\"\"\n",
    "    try:\n",
    "        loader = DirectoryLoader(\n",
    "            path=path,\n",
    "            glob=\"**/*.html\",\n",
    "            loader_cls=UnstructuredHTMLLoader\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        for d in docs:\n",
    "            d.metadata[\"source_site\"] = site_tag\n",
    "        print(f\"‚úÖ Loaded {len(docs)} HTML files from {site_tag}\")\n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading HTML from {path}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def load_pdf_folder(path, site_tag):\n",
    "    \"\"\"Load PDF files from a directory\"\"\"\n",
    "    try:\n",
    "        loader = DirectoryLoader(\n",
    "            path=path,\n",
    "            glob=\"**/*.pdf\",\n",
    "            loader_cls=PyPDFLoader\n",
    "        )\n",
    "        docs = loader.load()\n",
    "        for d in docs:\n",
    "            d.metadata[\"source_site\"] = site_tag\n",
    "        print(f\"‚úÖ Loaded {len(docs)} PDF files from {site_tag}\")\n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading PDF from {path}: {str(e)}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0cad54aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Directory ready: data_ingestion/mosdac_data/documents\n",
      "üìÅ Directory ready: data_ingestion/mosdac_data/web_pages\n",
      "üìÅ Directory ready: data_ingestion/vedas_data/web_pages\n",
      "üìÅ Directory ready: data_ingestion/bhuvan_data/web_pages\n",
      "\n",
      "üîÑ Loading documents from all platforms...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0x30f8cb for key /Im1027\n",
      "Multiple definitions in dictionary at byte 0x30f8dc for key /Im1027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 1265 PDF files from mosdac\n",
      "‚úÖ Loaded 220 TXT files from mosdac\n",
      "‚úÖ Loaded 0 HTML files from mosdac\n",
      "‚úÖ Loaded 233 TXT files from vedas\n",
      "‚úÖ Loaded 0 HTML files from vedas\n",
      "‚úÖ Loaded 6 TXT files from bhuvan\n",
      "‚úÖ Loaded 0 HTML files from bhuvan\n",
      "\n",
      "üìä Total documents loaded: 1724\n",
      "üìã Sample metadata: {'producer': 'Nitro PDF PrimoPDF', 'creator': 'PrimoPDF http://www.primopdf.com', 'creationdate': '2012-03-20T14:49:18-05:30', 'moddate': '2012-03-20T14:49:18-05:30', 'title': 'Microsoft Word - Analysed-Winds', 'author': 'admin', 'source': 'data_ingestion\\\\mosdac_data\\\\documents\\\\Analysed-Winds.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_site': 'mosdac'}\n",
      "üìà Documents by platform:\n",
      "   MOSDAC: 1485 documents\n",
      "   VEDAS: 233 documents\n",
      "   BHUVAN: 6 documents\n"
     ]
    }
   ],
   "source": [
    "# Root folders ‚Äì create directories if they don't exist\n",
    "ROOT_MOSDAC_PDF = \"data_ingestion/mosdac_data/documents\"\n",
    "ROOT_MOSDAC_WEB = \"data_ingestion/mosdac_data/web_pages\"\n",
    "ROOT_VEDAS_WEB = \"data_ingestion/vedas_data/web_pages\"\n",
    "ROOT_BHUVAN_WEB = \"data_ingestion/bhuvan_data/web_pages\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "directories = [ROOT_MOSDAC_PDF, ROOT_MOSDAC_WEB, ROOT_VEDAS_WEB, ROOT_BHUVAN_WEB]\n",
    "for directory in directories:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "    print(f\"üìÅ Directory ready: {directory}\")\n",
    "\n",
    "# Load documents from all platforms\n",
    "print(\"\\nüîÑ Loading documents from all platforms...\")\n",
    "\n",
    "mosdac_pdfs = load_pdf_folder(ROOT_MOSDAC_PDF, \"mosdac\")\n",
    "mosdac_txt = load_txt_folder(ROOT_MOSDAC_WEB, \"mosdac\")\n",
    "mosdac_html = load_html_folder(ROOT_MOSDAC_WEB, \"mosdac\")\n",
    "\n",
    "vedas_txt = load_txt_folder(ROOT_VEDAS_WEB, \"vedas\")\n",
    "vedas_html = load_html_folder(ROOT_VEDAS_WEB, \"vedas\")\n",
    "\n",
    "bhuvan_txt = load_txt_folder(ROOT_BHUVAN_WEB, \"bhuvan\")\n",
    "bhuvan_html = load_html_folder(ROOT_BHUVAN_WEB, \"bhuvan\")\n",
    "\n",
    "# Combine all documents\n",
    "documents_all = mosdac_pdfs + mosdac_txt + mosdac_html + vedas_txt + vedas_html + bhuvan_txt + bhuvan_html\n",
    "\n",
    "print(f\"\\nüìä Total documents loaded: {len(documents_all)}\")\n",
    "if documents_all:\n",
    "    print(\"üìã Sample metadata:\", documents_all[0].metadata)\n",
    "    \n",
    "    # Count by platform\n",
    "    platform_counts = {}\n",
    "    for doc in documents_all:\n",
    "        platform = doc.metadata.get('source_site', 'unknown')\n",
    "        platform_counts[platform] = platform_counts.get(platform, 0) + 1\n",
    "    \n",
    "    print(\"üìà Documents by platform:\")\n",
    "    for platform, count in platform_counts.items():\n",
    "        print(f\"   {platform.upper()}: {count} documents\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No documents found! Please add files to the data directories.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "61149bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Splitting documents into chunks...\n",
      "üìä Total chunks created: 4596\n",
      "üìà Chunks by platform:\n",
      "   MOSDAC: 3956 chunks\n",
      "   VEDAS: 634 chunks\n",
      "   BHUVAN: 6 chunks\n",
      "\n",
      "üìã Sample chunk metadata: {'producer': 'Nitro PDF PrimoPDF', 'creator': 'PrimoPDF http://www.primopdf.com', 'creationdate': '2012-03-20T14:49:18-05:30', 'moddate': '2012-03-20T14:49:18-05:30', 'title': 'Microsoft Word - Analysed-Winds', 'author': 'admin', 'source': 'data_ingestion\\\\mosdac_data\\\\documents\\\\Analysed-Winds.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1', 'source_site': 'mosdac'}\n",
      "üìù Sample content preview: GLOBAL ANALYSED OCEAN SURFACE WIND PRODUCTS \n",
      " \n",
      " \n",
      "Description \n",
      "The analysed winds have been generated at 0.5 \n",
      "0√ó0.5 0 interval over the global oceans. For the generation of \n",
      "these analysed winds produc...\n"
     ]
    }
   ],
   "source": [
    "# Text splitting with optimized parameters\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1200,\n",
    "    chunk_overlap=200,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    ")\n",
    "\n",
    "print(\"üîÑ Splitting documents into chunks...\")\n",
    "chunks_all = text_splitter.split_documents(documents_all)\n",
    "print(f\"üìä Total chunks created: {len(chunks_all)}\")\n",
    "\n",
    "# Analyze chunks by platform\n",
    "if chunks_all:\n",
    "    chunk_counts = {}\n",
    "    for chunk in chunks_all:\n",
    "        platform = chunk.metadata.get('source_site', 'unknown')\n",
    "        chunk_counts[platform] = chunk_counts.get(platform, 0) + 1\n",
    "    \n",
    "    print(\"üìà Chunks by platform:\")\n",
    "    for platform, count in chunk_counts.items():\n",
    "        print(f\"   {platform.upper()}: {count} chunks\")\n",
    "    \n",
    "    # Show sample chunk\n",
    "    first_chunk = chunks_all[0]\n",
    "    print(f\"\\nüìã Sample chunk metadata: {first_chunk.metadata}\")\n",
    "    print(f\"üìù Sample content preview: {first_chunk.page_content[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b73f14e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Setting up vector store...\n",
      "üóëÔ∏è Removing existing vector store...\n",
      "‚úÖ Old vector store removed\n",
      "üîÑ Creating new vector store...\n",
      "üìä Inserting 4596 chunks in batches of 64...\n",
      "‚úÖ Progress: 64/4596 chunks (1.4%)\n",
      "‚úÖ Progress: 128/4596 chunks (2.8%)\n",
      "‚úÖ Progress: 192/4596 chunks (4.2%)\n",
      "‚úÖ Progress: 256/4596 chunks (5.6%)\n",
      "‚úÖ Progress: 320/4596 chunks (7.0%)\n",
      "‚úÖ Progress: 384/4596 chunks (8.4%)\n",
      "‚úÖ Progress: 448/4596 chunks (9.7%)\n",
      "‚úÖ Progress: 512/4596 chunks (11.1%)\n",
      "‚úÖ Progress: 576/4596 chunks (12.5%)\n",
      "‚úÖ Progress: 640/4596 chunks (13.9%)\n",
      "‚úÖ Progress: 704/4596 chunks (15.3%)\n",
      "‚úÖ Progress: 768/4596 chunks (16.7%)\n",
      "‚úÖ Progress: 832/4596 chunks (18.1%)\n",
      "‚úÖ Progress: 896/4596 chunks (19.5%)\n",
      "‚úÖ Progress: 960/4596 chunks (20.9%)\n",
      "‚úÖ Progress: 1024/4596 chunks (22.3%)\n",
      "‚úÖ Progress: 1088/4596 chunks (23.7%)\n",
      "‚úÖ Progress: 1152/4596 chunks (25.1%)\n",
      "‚úÖ Progress: 1216/4596 chunks (26.5%)\n",
      "‚úÖ Progress: 1280/4596 chunks (27.9%)\n",
      "‚úÖ Progress: 1344/4596 chunks (29.2%)\n",
      "‚úÖ Progress: 1408/4596 chunks (30.6%)\n",
      "‚úÖ Progress: 1472/4596 chunks (32.0%)\n",
      "‚úÖ Progress: 1536/4596 chunks (33.4%)\n",
      "‚úÖ Progress: 1600/4596 chunks (34.8%)\n",
      "‚úÖ Progress: 1664/4596 chunks (36.2%)\n",
      "‚úÖ Progress: 1728/4596 chunks (37.6%)\n",
      "‚úÖ Progress: 1792/4596 chunks (39.0%)\n",
      "‚úÖ Progress: 1856/4596 chunks (40.4%)\n",
      "‚úÖ Progress: 1920/4596 chunks (41.8%)\n",
      "‚úÖ Progress: 1984/4596 chunks (43.2%)\n",
      "‚úÖ Progress: 2048/4596 chunks (44.6%)\n",
      "‚úÖ Progress: 2112/4596 chunks (46.0%)\n",
      "‚úÖ Progress: 2176/4596 chunks (47.3%)\n",
      "‚úÖ Progress: 2240/4596 chunks (48.7%)\n",
      "‚úÖ Progress: 2304/4596 chunks (50.1%)\n",
      "‚úÖ Progress: 2368/4596 chunks (51.5%)\n",
      "‚úÖ Progress: 2432/4596 chunks (52.9%)\n",
      "‚úÖ Progress: 2496/4596 chunks (54.3%)\n",
      "‚úÖ Progress: 2560/4596 chunks (55.7%)\n",
      "‚úÖ Progress: 2624/4596 chunks (57.1%)\n",
      "‚úÖ Progress: 2688/4596 chunks (58.5%)\n",
      "‚úÖ Progress: 2752/4596 chunks (59.9%)\n",
      "‚úÖ Progress: 2816/4596 chunks (61.3%)\n",
      "‚úÖ Progress: 2880/4596 chunks (62.7%)\n",
      "‚úÖ Progress: 2944/4596 chunks (64.1%)\n",
      "‚úÖ Progress: 3008/4596 chunks (65.4%)\n",
      "‚úÖ Progress: 3072/4596 chunks (66.8%)\n",
      "‚úÖ Progress: 3136/4596 chunks (68.2%)\n",
      "‚úÖ Progress: 3200/4596 chunks (69.6%)\n",
      "‚úÖ Progress: 3264/4596 chunks (71.0%)\n",
      "‚úÖ Progress: 3328/4596 chunks (72.4%)\n",
      "‚úÖ Progress: 3392/4596 chunks (73.8%)\n",
      "‚úÖ Progress: 3456/4596 chunks (75.2%)\n",
      "‚úÖ Progress: 3520/4596 chunks (76.6%)\n",
      "‚úÖ Progress: 3584/4596 chunks (78.0%)\n",
      "‚úÖ Progress: 3648/4596 chunks (79.4%)\n",
      "‚úÖ Progress: 3712/4596 chunks (80.8%)\n",
      "‚úÖ Progress: 3776/4596 chunks (82.2%)\n",
      "‚úÖ Progress: 3840/4596 chunks (83.6%)\n",
      "‚úÖ Progress: 3904/4596 chunks (84.9%)\n",
      "‚úÖ Progress: 3968/4596 chunks (86.3%)\n",
      "‚úÖ Progress: 4032/4596 chunks (87.7%)\n",
      "‚úÖ Progress: 4096/4596 chunks (89.1%)\n",
      "‚úÖ Progress: 4160/4596 chunks (90.5%)\n",
      "‚úÖ Progress: 4224/4596 chunks (91.9%)\n",
      "‚úÖ Progress: 4288/4596 chunks (93.3%)\n",
      "‚úÖ Progress: 4352/4596 chunks (94.7%)\n",
      "‚úÖ Progress: 4416/4596 chunks (96.1%)\n",
      "‚úÖ Progress: 4480/4596 chunks (97.5%)\n",
      "‚úÖ Progress: 4544/4596 chunks (98.9%)\n",
      "‚úÖ Progress: 4596/4596 chunks (100.0%)\n",
      "üéâ Vector store created successfully!\n",
      "üìä Final collection size: 4596\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings and vector store\n",
    "print(\"üîÑ Setting up vector store...\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Optional: start from a clean collection if you want to rebuild\n",
    "if os.path.exists(db_name):\n",
    "    print(\"üóëÔ∏è Removing existing vector store...\")\n",
    "    try:\n",
    "        old_store = Chroma(persist_directory=db_name, embedding_function=embeddings)\n",
    "        old_store.delete_collection()\n",
    "        print(\"‚úÖ Old vector store removed\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not remove old store: {e}\")\n",
    "\n",
    "print(\"üîÑ Creating new vector store...\")\n",
    "vectorstore = Chroma(persist_directory=db_name, embedding_function=embeddings)\n",
    "\n",
    "# Batch insert with progress tracking\n",
    "if chunks_all:\n",
    "    BATCH_SIZE = 64\n",
    "    total_chunks = len(chunks_all)\n",
    "    \n",
    "    print(f\"üìä Inserting {total_chunks} chunks in batches of {BATCH_SIZE}...\")\n",
    "    \n",
    "    for i in range(0, total_chunks, BATCH_SIZE):\n",
    "        batch = chunks_all[i:i+BATCH_SIZE]\n",
    "        vectorstore.add_documents(batch)\n",
    "        \n",
    "        progress = min(i + BATCH_SIZE, total_chunks)\n",
    "        percentage = (progress / total_chunks) * 100\n",
    "        print(f\"‚úÖ Progress: {progress}/{total_chunks} chunks ({percentage:.1f}%)\")\n",
    "    \n",
    "    final_count = vectorstore._collection.count()\n",
    "    print(f\"üéâ Vector store created successfully!\")\n",
    "    print(f\"üìä Final collection size: {final_count}\")\n",
    "else:\n",
    "    print(\"‚ùå No chunks to insert into vector store!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5ef09521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Setting up RAG chain...\n",
      "‚úÖ RAG chain setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM and retriever\n",
    "print(\"üîÑ Setting up RAG chain...\")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "# Enhanced prompt template\n",
    "rag_prompt_template = \"\"\"You are ASTROGEO AI, a specialized assistant for Indian space and earth observation data platforms.\n",
    "\n",
    "You have access to information from three major platforms:\n",
    "- MOSDAC: Meteorological & Oceanographic Satellite Data Archival Centre\n",
    "- VEDAS: Visualization of Earth observation Data and Archival System  \n",
    "- BHUVAN: Indian Geo-platform for visualization and analysis\n",
    "\n",
    "Instructions:\n",
    "1. Answer questions based on the provided context documents\n",
    "2. If information is available in the documents, provide detailed, comprehensive responses\n",
    "3. Always mention which platform(s) the information comes from (MOSDAC, VEDAS, or BHUVAN)\n",
    "4. If documents don't contain the answer, use general knowledge but clearly state this\n",
    "5. Be helpful, accurate, and technical when appropriate\n",
    "6. For data access questions, provide specific steps and requirements\n",
    "\n",
    "Context from documents:\n",
    "{context}\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "RAG_PROMPT = PromptTemplate.from_template(rag_prompt_template)\n",
    "\n",
    "# Create the conversational RAG chain\n",
    "rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={\"prompt\": RAG_PROMPT},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ RAG chain setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "484ca215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Smart bot function ready!\n"
     ]
    }
   ],
   "source": [
    "def is_relevant(question, docs):\n",
    "    \"\"\"Check if retrieved documents are relevant to the question\"\"\"\n",
    "    if not docs:\n",
    "        return False\n",
    "    \n",
    "    # Take the first document for relevance check\n",
    "    doc_content = docs[0].page_content[:1500] if docs else \"\"\n",
    "    \n",
    "    relevance_prompt = f\"\"\"Analyze if the following document content is relevant to answer the user's question.\n",
    "    Answer only \"Yes\" or \"No\".\n",
    "    \n",
    "    Question: {question}\n",
    "    \n",
    "    Document Content: {doc_content}\n",
    "    \n",
    "    Relevance Assessment:\"\"\"\n",
    "    \n",
    "    try:\n",
    "        check = llm.invoke(relevance_prompt)\n",
    "        return \"Yes\" in check.content\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Relevance check error: {e}\")\n",
    "        return True  # Default to using RAG if check fails\n",
    "\n",
    "# Fallback prompt for general knowledge responses\n",
    "fallback_prompt_template = \"\"\"You are ASTROGEO AI, a helpful assistant specializing in Indian space and earth observation platforms (MOSDAC, VEDAS, BHUVAN).\n",
    "\n",
    "The user's question doesn't seem to be directly related to the documents in your knowledge base, so please answer using your general knowledge while staying within your area of expertise.\n",
    "\n",
    "If the question is about:\n",
    "- Satellite data, remote sensing, or earth observation\n",
    "- Indian space programs (ISRO)\n",
    "- Weather and climate data\n",
    "- GIS and mapping\n",
    "- Data access and processing\n",
    "\n",
    "Please provide a helpful response. If it's completely outside your domain, politely redirect to your specializations.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Response:\"\"\"\n",
    "\n",
    "FALLBACK_PROMPT = PromptTemplate.from_template(fallback_prompt_template)\n",
    "\n",
    "def smart_bot(message, history):\n",
    "    \"\"\"Main chatbot function with smart routing\"\"\"\n",
    "    try:\n",
    "        # Retrieve relevant documents\n",
    "        docs = retriever.get_relevant_documents(message)\n",
    "        \n",
    "        # Check relevance and route accordingly\n",
    "        if docs and is_relevant(message, docs):\n",
    "            # Use RAG chain for document-based response\n",
    "            result = rag_chain.invoke({\"question\": message})\n",
    "            \n",
    "            # Add source information\n",
    "            sources = set()\n",
    "            if hasattr(result, 'source_documents') and result['source_documents']:\n",
    "                for doc in result['source_documents']:\n",
    "                    source_site = doc.metadata.get('source_site', 'unknown')\n",
    "                    sources.add(source_site.upper())\n",
    "            \n",
    "            response = result[\"answer\"]\n",
    "            if sources:\n",
    "                response += f\"\\n\\n*Sources: {', '.join(sources)}*\"\n",
    "            \n",
    "            return response\n",
    "        else:\n",
    "            # Use fallback for general knowledge\n",
    "            chain = FALLBACK_PROMPT | llm\n",
    "            result = chain.invoke({\"question\": message})\n",
    "            return result.content + \"\\n\\n*Note: This response is based on general knowledge as no relevant documents were found in the knowledge base.*\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Sorry, I encountered an error: {str(e)}. Please try rephrasing your question.\"\n",
    "\n",
    "print(\"‚úÖ Smart bot function ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3563fd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Launching ASTROGEO AI Chat Interface...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gradio\\chat_interface.py:345: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
      "  self.chatbot = Chatbot(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Launch error: Cannot find empty port in range: 7860-7860. You can specify a different port by setting the GRADIO_SERVER_PORT environment variable or passing the `server_port` parameter to `launch()`.\n",
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create and launch the Gradio interface\n",
    "print(\"üöÄ Launching ASTROGEO AI Chat Interface...\")\n",
    "\n",
    "# Custom CSS for better appearance\n",
    "custom_css = \"\"\"\n",
    ".gradio-container {\n",
    "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif !important;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Create the chat interface\n",
    "interface = gr.ChatInterface(\n",
    "    fn=smart_bot,\n",
    "    title=\"üõ∞Ô∏è ASTROGEO AI ‚Äî Multi-Platform Earth Observation Assistant\",\n",
    "    description=\"\"\"\n",
    "    **Powered by MOSDAC + VEDAS + BHUVAN Knowledge Base**\n",
    "    \n",
    "    Ask me about:\n",
    "    ‚Ä¢ MOSDAC satellite data and services\n",
    "    ‚Ä¢ VEDAS earth observation and visualization\n",
    "    ‚Ä¢ BHUVAN geospatial data and mapping\n",
    "    ‚Ä¢ Data access procedures and formats\n",
    "    ‚Ä¢ Indian space and remote sensing programs\n",
    "    \"\"\",\n",
    "    theme=\"soft\",\n",
    "    css=custom_css,\n",
    "    examples=[\n",
    "        \"What types of data are available in MOSDAC?\",\n",
    "        \"How do I access satellite imagery from VEDAS?\",\n",
    "        \"What mapping services does BHUVAN provide?\",\n",
    "        \"How can I download INSAT-3D data?\",\n",
    "        \"What are the data formats supported by these platforms?\",\n",
    "        \"Tell me about Indian earth observation satellites\"\n",
    "    ],\n",
    "    cache_examples=False,\n",
    "    analytics_enabled=False\n",
    ")\n",
    "\n",
    "# Launch the interface\n",
    "try:\n",
    "    interface.launch(\n",
    "        share=True,  # Creates public link\n",
    "        server_name=\"0.0.0.0\",  # Makes it accessible on network\n",
    "        server_port=7860,  # Default Gradio port\n",
    "        show_error=True\n",
    "    )\n",
    "    print(\"‚úÖ Interface launched successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Launch error: {e}\")\n",
    "    # Fallback launch without share\n",
    "    interface.launch(show_error=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c0b7b7f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üéâ ASTROGEO AI is ready!\n",
      "üìä System Statistics:\n",
      "   ‚Ä¢ Total documents: 1724\n",
      "   ‚Ä¢ Total chunks: 4596\n",
      "   ‚Ä¢ Vector store size: 4596\n",
      "   ‚Ä¢ Model: gpt-4o-mini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\AppData\\Local\\Temp\\ipykernel_636\\418538464.py:49: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  docs = retriever.get_relevant_documents(message)\n"
     ]
    }
   ],
   "source": [
    "# Test the system with sample queries\n",
    "def test_system():\n",
    "    \"\"\"Test the chatbot with sample queries\"\"\"\n",
    "    test_queries = [\n",
    "        \"What is MOSDAC?\",\n",
    "        \"How do I access VEDAS data?\",\n",
    "        \"What services does BHUVAN provide?\",\n",
    "        \"Tell me about INSAT satellites\",\n",
    "        \"What is the weather like today?\"  # This should trigger fallback\n",
    "    ]\n",
    "    \n",
    "    print(\"üß™ Testing the system with sample queries...\")\n",
    "    \n",
    "    for i, query in enumerate(test_queries, 1):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Test {i}: {query}\")\n",
    "        print('='*50)\n",
    "        \n",
    "        try:\n",
    "            response = smart_bot(query, [])\n",
    "            print(f\"Response: {response[:300]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Uncomment the line below to run tests\n",
    "# test_system()\n",
    "\n",
    "print(\"\\nüéâ ASTROGEO AI is ready!\")\n",
    "print(\"üìä System Statistics:\")\n",
    "print(f\"   ‚Ä¢ Total documents: {len(documents_all)}\")\n",
    "print(f\"   ‚Ä¢ Total chunks: {len(chunks_all)}\")\n",
    "print(f\"   ‚Ä¢ Vector store size: {vectorstore._collection.count()}\")\n",
    "print(f\"   ‚Ä¢ Model: {MODEL}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c4818ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "##heyy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
