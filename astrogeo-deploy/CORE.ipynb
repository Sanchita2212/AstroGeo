{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd1ce2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ankit\\AstroGeoAI\\AstroGeoAI-Core\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Core libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# Core Python imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "#version1223\n",
    "\n",
    "# Core ML/Data imports\n",
    "import gradio as gr\n",
    "import numpy as np\n",
    "\n",
    "# LangChain - RAG knowledge base\n",
    "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader, TextLoader, UnstructuredHTMLLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "print(\"✅ Core libraries imported.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55699bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using Hugging Face GPT-OSS 120B (Free)\n",
      "✅ Hugging Face Token set: hf_GNQRSqB...mvKy\n"
     ]
    }
   ],
   "source": [
    "# Set API Keys - Using HF GPT-OSS (Free) instead of OpenAI\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv(override=True)\n",
    "\n",
    "# ✅ Read Hugging Face Token directly from .env\n",
    "hf_key = os.getenv(\"HF_TOKEN\")  # Make sure .env has HF_TOKEN=your_token\n",
    "\n",
    "# Optionally set it to os.environ under the name Hugging Face expects\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = hf_key  \n",
    "\n",
    "print(\"✅ Using Hugging Face GPT-OSS 120B (Free)\")\n",
    "\n",
    "# Verify HF token\n",
    "if hf_key and hf_key.startswith(\"hf_\"):\n",
    "    print(f\"✅ Hugging Face Token set: {hf_key[:10]}...{hf_key[-4:]}\")\n",
    "else:\n",
    "    print(\"⚠️ Hugging Face Token invalid or missing! Check your .env file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32f2e81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Data directories ready.\n"
     ]
    }
   ],
   "source": [
    "# Directory structure setup\n",
    "DATA_DIRS = {\n",
    "    \"mosdac_pdf\": \"data_ingestion/mosdac_data/documents\",\n",
    "    \"mosdac_web\": \"data_ingestion/mosdac_data/web_pages\",\n",
    "    \"vedas_web\": \"data_ingestion/vedas_data/web_pages\",\n",
    "    \"bhuvan_web\": \"data_ingestion/bhuvan_data/web_pages\",\n",
    "}\n",
    "\n",
    "# Ensure directories exist\n",
    "for d in DATA_DIRS.values():\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "print(\"📁 Data directories ready.\")\n",
    "\n",
    "# Define universal loader\n",
    "def load_docs(path, glob_pattern, loader_func, tag):\n",
    "    try:\n",
    "        loader = DirectoryLoader(path=path, glob=glob_pattern, loader_cls=loader_func)\n",
    "        docs = loader.load()\n",
    "        for d in docs:\n",
    "            d.metadata[\"source_site\"] = tag\n",
    "        print(f\"✅ Loaded {len(docs)} files from {tag}.\")\n",
    "        return docs\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading from {path}: {str(e)}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb67dbfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Multiple definitions in dictionary at byte 0x30f8cb for key /Im1027\n",
      "Multiple definitions in dictionary at byte 0x30f8dc for key /Im1027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1265 files from mosdac.\n",
      "✅ Loaded 220 files from mosdac.\n",
      "✅ Loaded 0 files from mosdac.\n",
      "✅ Loaded 233 files from vedas.\n",
      "✅ Loaded 0 files from vedas.\n",
      "✅ Loaded 6 files from bhuvan.\n",
      "✅ Loaded 0 files from bhuvan.\n",
      "📊 Total documents loaded: 1724\n"
     ]
    }
   ],
   "source": [
    "# Load each type by platform\n",
    "mosdac_pdfs = load_docs(DATA_DIRS['mosdac_pdf'], \"**/*.pdf\", PyPDFLoader, \"mosdac\")\n",
    "mosdac_txt = load_docs(DATA_DIRS['mosdac_web'], \"**/*.txt\", lambda p: TextLoader(p, encoding=\"utf-8\"), \"mosdac\")\n",
    "mosdac_html = load_docs(DATA_DIRS['mosdac_web'], \"**/*.html\", UnstructuredHTMLLoader, \"mosdac\")\n",
    "vedas_txt = load_docs(DATA_DIRS['vedas_web'], \"**/*.txt\", lambda p: TextLoader(p, encoding=\"utf-8\"), \"vedas\")\n",
    "vedas_html = load_docs(DATA_DIRS['vedas_web'], \"**/*.html\", UnstructuredHTMLLoader, \"vedas\")\n",
    "bhuvan_txt = load_docs(DATA_DIRS['bhuvan_web'], \"**/*.txt\", lambda p: TextLoader(p, encoding=\"utf-8\"), \"bhuvan\")\n",
    "bhuvan_html = load_docs(DATA_DIRS['bhuvan_web'], \"**/*.html\", UnstructuredHTMLLoader, \"bhuvan\")\n",
    "\n",
    "# Combine\n",
    "documents_all = mosdac_pdfs + mosdac_txt + mosdac_html + vedas_txt + vedas_html + bhuvan_txt + bhuvan_html\n",
    "print(f\"📊 Total documents loaded: {len(documents_all)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b5d6483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Total chunks created: 4596\n"
     ]
    }
   ],
   "source": [
    "# Split into chunks for vector DB\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=200)\n",
    "chunks_all = splitter.split_documents(documents_all)\n",
    "print(f\"📊 Total chunks created: {len(chunks_all)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3f791172",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🗑️ Successfully deleted vector_db\n",
      "✅ Fresh vector store created\n",
      "📊 Adding 100 MOSDAC/VEDAS/BHUVAN chunks...\n",
      "📊 Processing 10 documents for embeddings...\n",
      "✅ Processed 10/10 embeddings\n",
      "✅ Added batch 1: 10/100\n",
      "📊 Processing 10 documents for embeddings...\n",
      "✅ Processed 10/10 embeddings\n",
      "✅ Added batch 2: 20/100\n",
      "📊 Processing 10 documents for embeddings...\n",
      "✅ Processed 10/10 embeddings\n",
      "✅ Added batch 3: 30/100\n",
      "📊 Processing 10 documents for embeddings...\n",
      "✅ Processed 10/10 embeddings\n",
      "✅ Added batch 4: 40/100\n",
      "📊 Processing 10 documents for embeddings...\n",
      "✅ Processed 10/10 embeddings\n",
      "✅ Added batch 5: 50/100\n",
      "📊 Processing 10 documents for embeddings...\n",
      "✅ Processed 10/10 embeddings\n",
      "✅ Added batch 6: 60/100\n",
      "📊 Processing 10 documents for embeddings...\n",
      "✅ Processed 10/10 embeddings\n",
      "✅ Added batch 7: 70/100\n",
      "📊 Processing 10 documents for embeddings...\n",
      "✅ Processed 10/10 embeddings\n",
      "✅ Added batch 8: 80/100\n",
      "📊 Processing 10 documents for embeddings...\n",
      "✅ Processed 10/10 embeddings\n",
      "✅ Added batch 9: 90/100\n",
      "📊 Processing 10 documents for embeddings...\n",
      "✅ Processed 10/10 embeddings\n",
      "✅ Added batch 10: 100/100\n",
      "🎉 Vector store created with 100 MOSDAC/VEDAS/BHUVAN documents!\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Vector Store with RAG - WINDOWS PERMISSION FIX\n",
    "import shutil\n",
    "import os\n",
    "import stat\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Step 1: Windows-compatible folder deletion\n",
    "def force_delete_folder(path):\n",
    "    \"\"\"Force delete folder on Windows with permission fixes\"\"\"\n",
    "    def handle_remove_readonly(func, path, exc):\n",
    "        if os.path.exists(path):\n",
    "            os.chmod(path, stat.S_IWRITE)\n",
    "            func(path)\n",
    "    \n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            shutil.rmtree(path, onerror=handle_remove_readonly)\n",
    "            print(f\"🗑️ Successfully deleted {path}\")\n",
    "        else:\n",
    "            print(f\"📁 {path} doesn't exist - creating fresh\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Manual deletion needed: {e}\")\n",
    "        print(\"💡 Solution: Restart your Jupyter kernel, then run this cell again\")\n",
    "\n",
    "# Delete old vector DB\n",
    "db_name = \"vector_db\"\n",
    "force_delete_folder(db_name)\n",
    "\n",
    "# Step 2: Custom HF Embeddings with consistent dimensions\n",
    "class HFEmbeddings:\n",
    "    def __init__(self, client):\n",
    "        self.client = client\n",
    "        self.model = \"sentence-transformers/all-mpnet-base-v2\"  # 768 dimensions\n",
    "        \n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = []\n",
    "        print(f\"📊 Processing {len(texts)} documents for embeddings...\")\n",
    "        \n",
    "        for i, text in enumerate(texts):\n",
    "            try:\n",
    "                # Extract page_content if it's a Document object\n",
    "                content = text.page_content if hasattr(text, 'page_content') else str(text)\n",
    "                content = content[:500]  # Limit text length for free tier\n",
    "                \n",
    "                # Get embedding from HF\n",
    "                emb = self.client.feature_extraction(content, model=self.model)\n",
    "                \n",
    "                # Handle response format\n",
    "                if isinstance(emb, list) and len(emb) > 0:\n",
    "                    embedding = emb[0] if isinstance(emb[0], list) else emb\n",
    "                else:\n",
    "                    embedding = [0.1] * 768  # Fallback\n",
    "                    \n",
    "                embeddings.append(embedding)\n",
    "                \n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(f\"✅ Processed {i + 1}/{len(texts)} embeddings\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"⚠️ Error for text {i}: {e}\")\n",
    "                embeddings.append([0.1] * 768)  # Fallback\n",
    "                \n",
    "        return embeddings\n",
    "        \n",
    "    def embed_query(self, text):\n",
    "        try:\n",
    "            text = text[:500]  # Limit length\n",
    "            emb = self.client.feature_extraction(text, model=self.model)\n",
    "            return emb[0] if isinstance(emb, list) and isinstance(emb, list) else emb\n",
    "        except:\n",
    "            return [0.1] * 768\n",
    "\n",
    "# Step 3: Initialize client and embeddings\n",
    "client = InferenceClient(token=os.getenv('HF_TOKEN'))\n",
    "embeddings = HFEmbeddings(client)\n",
    "\n",
    "# Step 4: Create fresh vector store\n",
    "try:\n",
    "    vectorstore = Chroma(persist_directory=db_name, embedding_function=embeddings)\n",
    "    print(\"✅ Fresh vector store created\")\n",
    "    \n",
    "    # Step 5: Add your MOSDAC/VEDAS/BHUVAN documents in batches\n",
    "    if 'chunks_all' in globals() and chunks_all:\n",
    "        BATCH_SIZE = 10  # Small batches for free tier\n",
    "        total_chunks = min(100, len(chunks_all))  # Limit for testing\n",
    "        \n",
    "        print(f\"📊 Adding {total_chunks} MOSDAC/VEDAS/BHUVAN chunks...\")\n",
    "        \n",
    "        for i in range(0, total_chunks, BATCH_SIZE):\n",
    "            batch = chunks_all[i:i+BATCH_SIZE]\n",
    "            \n",
    "            try:\n",
    "                vectorstore.add_documents(batch)\n",
    "                progress = min(i + BATCH_SIZE, total_chunks)\n",
    "                print(f\"✅ Added batch {i//BATCH_SIZE + 1}: {progress}/{total_chunks}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"❌ Batch {i//BATCH_SIZE + 1} failed: {e}\")\n",
    "                continue\n",
    "        \n",
    "        print(f\"🎉 Vector store created with {total_chunks} MOSDAC/VEDAS/BHUVAN documents!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ No chunks_all found. Please run document loading cells first.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Vector store creation failed: {e}\")\n",
    "    print(\"💡 Try restarting your kernel and running this cell again\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b3377016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Free GPT-OSS 120B setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Setup FREE GPT-OSS 120B from Hugging Face\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "class FreeGPTOSS:\n",
    "    def __init__(self, hf_token):\n",
    "        self.client = InferenceClient(token=hf_token)\n",
    "        self.model = \"openai/gpt-oss-120b\"  # Free GPT-OSS model\n",
    "        \n",
    "    def invoke(self, prompt):\n",
    "        try:\n",
    "            response = self.client.text_generation(\n",
    "                prompt=prompt,\n",
    "                model=self.model,\n",
    "                max_new_tokens=500,\n",
    "                temperature=0.7\n",
    "            )\n",
    "            # Create response object similar to OpenAI format\n",
    "            class Response:\n",
    "                def __init__(self, content):\n",
    "                    self.content = content\n",
    "            return Response(response)\n",
    "        except Exception as e:\n",
    "            return Response(f\"Error: {str(e)}\")\n",
    "\n",
    "# Initialize free LLM\n",
    "llm = FreeGPTOSS(os.getenv('HF_TOKEN'))\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Simplified RAG without LangChain memory (to avoid OpenAI dependency)\n",
    "def simple_rag_query(question):\n",
    "    try:\n",
    "        docs = retriever.get_relevant_documents(question)\n",
    "        context = \"\\n\".join([doc.page_content for doc in docs[:3]])\n",
    "        \n",
    "        prompt = f\"\"\"You are ASTROGEO AI, a specialist in Indian space and earth observation platforms.\n",
    "\n",
    "Context from documents:\n",
    "{context}\n",
    "\n",
    "User Question: {question}\n",
    "\n",
    "Provide a detailed response based on the context above.\"\"\"\n",
    "        \n",
    "        response = llm.invoke(prompt)\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\"\n",
    "\n",
    "print(\"✅ Free GPT-OSS 120B setup complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b5464ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ultra-fast free image generator ready!\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: FASTEST Free Space Image Generator - REPLACE YOUR CURRENT ONE\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "class UltraFastImageGen:\n",
    "    def __init__(self, hf_token):\n",
    "        self.client = InferenceClient(token=hf_token)\n",
    "        self.model = \"runwayml/stable-diffusion-v1-5\"  # PROVEN working free model\n",
    "    \n",
    "    def generate(self, prompt):\n",
    "        # Auto-enhance for space images\n",
    "        enhanced = f\"{prompt}, space photography, ultra detailed, cinematic, 8K, photorealistic\"\n",
    "        \n",
    "        try:\n",
    "            response = self.client.text_to_image(\n",
    "                prompt=enhanced[:400],  # Keep short for speed\n",
    "                model=self.model\n",
    "            )\n",
    "            return response, \"✅ Generated with Stable Diffusion v1.5 (FREE)\"\n",
    "        except Exception as e:\n",
    "            return None, f\"❌ Failed: {str(e)}\"\n",
    "\n",
    "# REPLACE your image_generator with this:\n",
    "image_generator = UltraFastImageGen(os.getenv('HF_TOKEN'))\n",
    "print(\"✅ Ultra-fast free image generator ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "753e736a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Speech-to-Text (Whisper) ready\n"
     ]
    }
   ],
   "source": [
    "# Speech-to-Text using Hugging Face Whisper\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "class AstroGeoASR:\n",
    "    def __init__(self, hf_token):\n",
    "        self.client = InferenceClient(token=hf_token)\n",
    "        self.models = {\n",
    "            \"whisper\": \"openai/whisper-large-v3\",\n",
    "            \"whisper_small\": \"openai/whisper-small\"\n",
    "        }\n",
    "        \n",
    "    def transcribe(self, audio_file, model=\"whisper\"):\n",
    "        try:\n",
    "            with open(audio_file, \"rb\") as f:\n",
    "                result = self.client.automatic_speech_recognition(\n",
    "                    data=f.read(),\n",
    "                    model=self.models[model]\n",
    "                )\n",
    "            return result.get(\"text\", \"No transcription available\")\n",
    "        except Exception as e:\n",
    "            return f\"❌ ASR failed: {str(e)}\"\n",
    "\n",
    "asr = AstroGeoASR(os.getenv('HF_TOKEN'))\n",
    "print(\"✅ Speech-to-Text (Whisper) ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "807c189d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Updated to conversational smart bot style!\n",
      "✅ Responses will be natural paragraphs, not tables!\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Smart Bot with Conversational Style\n",
    "def smart_bot(message, history, asr_result=None):\n",
    "    \"\"\"Smart conversational bot - natural responses like a normal chatbot\"\"\"\n",
    "    \n",
    "    # Initialize response\n",
    "    response = \"\"\n",
    "    \n",
    "    try:\n",
    "        # Add ASR transcription if available\n",
    "        if asr_result:\n",
    "            response += f\"[From audio]: {asr_result}\\n\\n\"\n",
    "        \n",
    "        # Use conversational format with improved prompt\n",
    "        from huggingface_hub import InferenceClient\n",
    "        client = InferenceClient(token=os.getenv('HF_TOKEN'))\n",
    "        \n",
    "        # SMART CONVERSATIONAL SYSTEM PROMPT\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"openai/gpt-oss-120b\",\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\", \n",
    "                    \"content\": \"\"\"You are ASTROGEO AI, a smart and knowledgeable assistant specializing in space technology and earth observation. \n",
    "\n",
    "Instructions for responses:\n",
    "- Be conversational and friendly, like a smart expert friend\n",
    "- Use natural paragraph format, avoid tables or bullet points\n",
    "- Give comprehensive but readable explanations\n",
    "- Include technical details when relevant but explain them clearly\n",
    "- Be specific and accurate about MOSDAC, VEDAS, BHUVAN, satellites, and space technology\n",
    "- Sound professional yet approachable\"\"\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": message\n",
    "                }\n",
    "            ],\n",
    "            max_tokens=400,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        # Extract natural response\n",
    "        ai_response = completion.choices[0].message.content\n",
    "        response += ai_response\n",
    "        \n",
    "        return response\n",
    "        \n",
    "    except Exception as e:\n",
    "        return f\"I'm ASTROGEO AI, your smart space technology assistant. I can help with questions about satellites, MOSDAC, VEDAS, BHUVAN, and space observation. Error: {str(e)}\"\n",
    "\n",
    "print(\"✅ Updated to conversational smart bot style!\")\n",
    "print(\"✅ Responses will be natural paragraphs, not tables!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "53c89cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Complete multimodal interface ready (100% FREE)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ankit\\AppData\\Local\\Temp\\ipykernel_392\\3961839439.py:15: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot = gr.Chatbot(label=\"🛰️ ASTROGEO AI\", height=500)\n"
     ]
    }
   ],
   "source": [
    "# Complete Gradio Interface with Multi-Modal Support\n",
    "def create_astrogeo_interface():\n",
    "    with gr.Blocks(title=\"🛰️ ASTROGEO AI Pro - Free Edition\") as iface:\n",
    "        gr.Markdown(\"\"\"\n",
    "        # 🛰️ ASTROGEO AI Pro - Free Edition\n",
    "        ## Multi-Modal System: Text + Image + Speech\n",
    "        **Powered by Free Hugging Face Models**\n",
    "        - 🤖 **Text:** GPT-OSS 120B (Free)\n",
    "        - 🎨 **Images:** FLUX & SDXL (Free)\n",
    "        - 🎤 **Speech:** Whisper Large (Free)\n",
    "        \"\"\")\n",
    "\n",
    "        with gr.Row():\n",
    "            with gr.Column(scale=2):\n",
    "                chatbot = gr.Chatbot(label=\"🛰️ ASTROGEO AI\", height=500)\n",
    "                \n",
    "                with gr.Row():\n",
    "                    msg = gr.Textbox(\n",
    "                        placeholder=\"Ask about MOSDAC, VEDAS, BHUVAN...\",\n",
    "                        label=\"Text Input\",\n",
    "                        scale=3\n",
    "                    )\n",
    "                    audio_in = gr.Audio(\n",
    "                        sources=[\"microphone\"],\n",
    "                        type=\"filepath\",\n",
    "                        label=\"🎤 Speech Input\",\n",
    "                        scale=1\n",
    "                    )\n",
    "                \n",
    "                with gr.Row():\n",
    "                    model_choice = gr.Dropdown(\n",
    "                        choices=[\"flux\", \"sdxl\"],\n",
    "                        value=\"flux\",\n",
    "                        label=\"🎨 Image Model\"\n",
    "                    )\n",
    "                    send_btn = gr.Button(\"🚀 Send\", variant=\"primary\")\n",
    "                    clear_btn = gr.Button(\"🗑️ Clear\", variant=\"secondary\")\n",
    "\n",
    "            with gr.Column(scale=1):\n",
    "                gen_img = gr.Image(label=\"🖼️ Generated Image\", visible=True)\n",
    "                status = gr.Textbox(label=\"📊 Status\", visible=True, lines=3)\n",
    "\n",
    "        # Processing function\n",
    "        def process_all(msg_text, chat_hist, audio_path, img_model):\n",
    "            try:\n",
    "                # Handle speech input\n",
    "                asr_text = None\n",
    "                if audio_path:\n",
    "                    asr_text = asr.transcribe(audio_path)\n",
    "                \n",
    "                # Use text or speech input\n",
    "                input_text = msg_text or (asr_text if asr_text else \"\")\n",
    "                if not input_text:\n",
    "                    return chat_hist, \"\", None, None, \"❌ No input provided\"\n",
    "                \n",
    "                # Get text response\n",
    "                response = smart_bot(input_text, chat_hist, asr_text)\n",
    "                \n",
    "                # Generate image if requested\n",
    "                img, img_status = None, \"No image generation requested\"\n",
    "                if any(word in input_text.lower() for word in [\"image\", \"visualize\", \"generate\", \"show\", \"create\"]):\n",
    "                    img, img_status = image_gen.generate(input_text, img_model)\n",
    "                \n",
    "                # Update chat\n",
    "                chat_hist = chat_hist + [(input_text, response)]\n",
    "                \n",
    "                # Status update\n",
    "                status_msg = f\"✅ Response generated\\n🎨 {img_status}\\n📊 Using Free HF Models\"\n",
    "                \n",
    "                return chat_hist, \"\", None, img, status_msg\n",
    "                \n",
    "            except Exception as e:\n",
    "                error_msg = f\"❌ Error: {str(e)}\"\n",
    "                chat_hist = chat_hist + [(input_text or \"Error\", error_msg)]\n",
    "                return chat_hist, \"\", None, None, f\"❌ Error: {str(e)}\"\n",
    "\n",
    "        # Event handlers\n",
    "        send_btn.click(\n",
    "            process_all,\n",
    "            inputs=[msg, chatbot, audio_in, model_choice],\n",
    "            outputs=[chatbot, msg, audio_in, gen_img, status]\n",
    "        )\n",
    "        \n",
    "        clear_btn.click(\n",
    "            fn=lambda: ([], \"\", None, None, \"Ready for questions...\"),\n",
    "            outputs=[chatbot, msg, audio_in, gen_img, status]\n",
    "        )\n",
    "\n",
    "    return iface\n",
    "\n",
    "# Create interface\n",
    "iface = create_astrogeo_interface()\n",
    "print(\"✅ Complete multimodal interface ready (100% FREE)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f28a1230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://ae26a409a3f45c230d.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://ae26a409a3f45c230d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Launch on a random port for safety\n",
    "iface.launch(\n",
    "    share=True,\n",
    "    inbrowser=True,\n",
    "    show_error=True\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
